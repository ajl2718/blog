{
  "hash": "381d8db885cf293feaf5dac88e9d4d43",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Larger than memory datasets in Python using Polars\"\nauthor: \"Alex Lee\"\ndate: \"2024-03-31\"\ncategories: [python, polars, duckdb, large data]\nimage: GettyImages-1296252531.jpg\nformat:\n  html:\n    code-links:\n      - text: Polars vs Pandas\n        icon: file-code\n        href: https://github.com/saunteringcat/python_learning/blob/master/polars_vs_pandas_020424.ipynb\n---\n\nWhile Pandas is still the go-to Python package for data analysis, there are now a new generation of data analysis tools which are much faster and better suited to working with data that is larger than the available memory. It isn't unusual to encounter a dataset around 10Gb in size which is usually just large enough to be awkward to work with on a single machine.\n\nThis article is a summary of some tinkering with these tools. These turn out to be very useful for the kinds of data that I use in my day job. The goal is to answer the questions:\n\n- Why use a package such as Polars instead of Pandas?\n- How does the speed compare?\n- How to write Polars code?\n- When to use one and not the other?\n\nIn addition to Polars, there are other tools in this area, including [DuckDB](https://www.duckdb.org) and [Ibis](https://ibis-project.org). And if you are using R there is [an excellent tutorial](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/) on how to use the [Arrow](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/) package for working with large datasets.\n\n## Querying data in Pandas: the messy way and the pretty way\n\nFirst let's download a largish dataset. [GNAF Core](https://geoscape.com.au/data/g-naf-core/) is a quarterly-updated dataset containing all street addresses in Australia. It's a single csv (well actually psv file) around 3.2Gb in size, so can fit in memory and analysed using Pandas.\n\nLet's load data and carry out some simple transformations.\n\n```default\nimport pandas as pd\ngnaf = pd.read_csv('gnaf_core.psv', sep='|')\ngnaf.shape\n```\n\n::: {#b8722915 .cell execution_count=1}\n``` {.python .cell-code}\na, b = 3, 5\na+b\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n8\n```\n:::\n:::\n\n\nThis is a dataframe with 15.6 million rows and 27 columns. Let's group the number of addresses in postcode 3000 (City of Melbourne) by street name and count how many there are.\n\nOne way to do this is as follows:\n\n```default\n# filter to postcode 3000\ngnaf_melbourne = gnaf[gnaf['POSTCODE'] == 3000]\n\n# count the number of addresses per street in postcode 3000\nmelbourne_street_counts = gnaf_melbourne.groupby('STREET_NAME').size()\n\n# sort the data in descending order by the number of addresses\nmelbourne_street_counts.sort_values(ascending=False)\n```\n\nWe should see that the top 5 streets are *LONSDALE*, *ABECKETT*, *COLLINS*, *LITTLE LONSDALE*, *LA TROBE* which have over 7000 addresses, while streets like *HEAPE*, *HARWOOD*, *RACING CLUB* and *DOMAIN* each have only one address.\n\nA nicer way of writing this code is to chain these statements into a single query. I learned this from reading [Matt Harrison's](https://twitter.com/__mharrison__?lang=en) tweets and articles.\n\n```default\ngnaf.query('POSTCODE == 3000').groupby('STREET_NAME').size().sort_values(ascending=False)\n```\nNotice that this uses the **query** method to filter the addresses and then a sequence of methods to carry the remaining operations. In a slightly more readable form\n\n```default\n(\n    gnaf\n    .query('POSTCODE == 3000')\n    .groupby('STREET_NAME')\n    .size()\n    .sort_values(ascending=False)\n)\n```\nThis sequence of transformation now reads as a single 'recipe' with each transformation appearing in a single line, making it all much easier to interpret. It more closely resembles **dplyr** code in R and as we see, Polars code. It also avoids creating unneccessary variables and memory in the intermediate steps. So far, so good.\n\nLet's answer another question: What is the distribution of starting letters for streets in New South Wales?\n\n```default\n(\n    gnaf\n    .query('STATE == \"NSW\"')\n    .STREET_NAME\n    .str.slice(0, 1)\n    .value_counts()\n)\n```\n\nAnd the most common starting letter for a street name is B with 383,483 addresses. So now we can write slick, easy to read queries in Pandas, and this leads us to the main subject of this article: Polars.\n\n## Why Polars?\nI have only recently started playing around with Polars but it's proving to be a better alternative to Pandas for large datasets. I am definitely not an expert in this area, and there are other packages such as [Ibis](https://ibis-project.org/) and [DuckDB](https://duckdb.org/) are being widely used as well.\n\nThe API for Polars may feel a bit unintuitive at first, but less so once you're familiar with the method chaining approach to writing Pandas transformations as described above. In any case, it's all well documented on the [Polars webpage](https://pola.rs/)\n\n```default\nimport polars as pl\n\ngnaf = pl.read_csv('GNAF_CORE.psv', separator='|', infer_schema_length=10000)\ngnaf.shape\n```\n\nThe first thing you should notice is the speed in loading the csv. We'll to a comparison with Pandas later.\n\n```default\n(\n    gnaf\n    .filter(pl.col(\"POSTCODE\") == 3000)\n    .group_by(\"STREET_NAME\")\n    .count()\n    .sort('count', descending=True)\n)\n```\n\nAside from some annoying differences in syntax (e.g., using ```descending``` instead of ```ascending``` to sort the dataframe), the overall code looks very similar. And now for the second transformation that counts the number of street names in New South Wales starting with different letters.\n\n```default\n(\n    gnaf\n    .filter(pl.col('STATE') == \"NSW\")\n    .select(pl.col(\"STREET_NAME\"))\n    .with_columns(pl.col(\"STREET_NAME\").str.slice(0, 1).alias(\"letter\"))\n    .group_by('letter')\n    .count()\n    .sort('count', descending=True)\n)\n```\n\nOk, not quite as elegant, especially with the ```with_columns``` syntax but it gives the same results.\n\n## Comparing the speeds\nSo loading the csv file was faster, let's compare how fast\n\n```default\nt1 = time()\ngnaf = pl.read_csv(filename, separator='|', infer_schema_length=10000)\nt2 = time()\nprint(t2-t1)\n\nt1 = time()\ngnaf = pd.read_csv(filename, sep='|')\nt2 = time()\nprint(t2-t1)\n```\n\nOn my Macbook M3, Polars takes around 1.7seconds and Pandas around 39.1 seconds, so Polars is over 20 times faster at loading this csv file!\n\n```default\nt1 = time()\n(\n    gnaf\n    .filter(pl.col('STATE') == \"NSW\")\n    .select(pl.col(\"STREET_NAME\"))\n    .with_columns(pl.col(\"STREET_NAME\").str.slice(0, 1).alias(\"letter\"))\n    .group_by('letter')\n    .count()\n    .sort('count', descending=True)\n)\nt2 = time()\nprint(t2-t1)\n\nt1 = time()\n(\n    gnaf\n    .query('STATE == \"NSW\"')\n    .STREET_NAME\n    .str.slice(0, 1)\n    .value_counts()\n)\nt2 = time()\nprint(t2-t1)\n```\nAround 1.3s in Polars and 2.4s in Pandas.\n\n```default\nt1 = time()\n(\n    gnaf\n    .filter(pl.col(\"POSTCODE\") == 3000)\n    .group_by(\"STREET_NAME\")\n    .len()\n    .sort('len', descending=True)\n)\nt2 = time()\nprint(t2 - t1)\n\ngnaf = pd.read_csv(filename, sep='|')\n\nt1 = time()\nresults = (\n    gnaf\n    .query('POSTCODE == 3000')\n    .groupby('STREET_NAME')\n    .size()\n    .sort_values(ascending=False)\n)\nt2 = time()\nprint(t2-t1)\n```\n\n1.39s in Polars and 0.43s in Pandas. So Pandas is the winner here.\n\n## Another speed up: parquet files\n\nRather than using csv files we can convert them to parquet. This is a binary file format that represents tabular datasets as columns rather than rows; both of these properties result in smaller file sizes that are quicker to load. A better explanation of why this is the case can be found in [this article](https://wesmckinney.com/blog/python-parquet-multithreading/) by Wes McKinney, the creator of Pandas.\n\nThe original file is 3.2Gb in csv format but when converted to parquet it's 970Mb. This is a pretty decent saving on space. From experience Parquet files can be up to 10% the size of csv file.\n\n```default\n# load in pandas\nt1 = time()\ngnaf = pd.read_parquet(filename)\nt2 = time()\nprint(t2-t1)\n\n# load in polars\nt1 = time()\ngnaf = pl.read_parquet(filename)\nt2 = time()\nprint(t2-t1)\n```\n1.3s in Polars and 14.1s in Pandas. \n\n## Lazy evaluation\nThe real benefit of using polars is that queries can be executed using so-called *lazy evaluation*, rather than loading the data into memory first.\n\nThis involves the use of ```scan_parquet``` or ```scan_csv``` which essentially creates a pointer to the file on the disk.\n\n```default\ngnaf = pl.scan_parquet(filename)\nt1 = time()\n(\n    gnaf\n    .filter(pl.col(\"POSTCODE\") == 3000)\n    .group_by(\"STREET_NAME\")\n    .len()\n    .sort('len', descending=True)\n    .collect()\n)\nt2 = time()\nprint(t2 - t1)\n```\n\nThe only difference is that a ```collect()``` method is appended to the end of the query so that it actually runs. The result: only 0.08 seconds! I was surprised about how much faster this is. Let's see how fast the earlier query runs:\n\n```default\nt1 = time()\n(\n    gnaf\n    .filter(pl.col('STATE') == \"NSW\")\n    .select(pl.col(\"STREET_NAME\"))\n    .with_columns(pl.col(\"STREET_NAME\").str.slice(0, 1).alias(\"letter\"))\n    .group_by('letter')\n    .len()\n    .sort('len', descending=True)\n    .collect()\n)\nt2 = time()\nprint(t2-t1)\n```\nWhich takes 0.25 seconds.\n\n## Larger than memory datasets\n\nSince the main point of this article is to better understand how to work with larger than memory data in Python, let's look at a collection of datasets from [New York City taxi trips](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). From 2010 to 2023 is around 25Gb of data.\n\n## Lessons\nTo summarise:\n\n- Use **parquet files rather than csv** where possible to reduce storage and speed up querying.\n\n- If using pandas for data analysis, **write data transformations as single queries involving method chaining**, rather than in multiple steps. This not only makes code easier to read, but easier to debug and removes so-called 'side-effects.'\n\n- **Switch to a package like Polars** to significantly speed up data analysis workflows.\n\n- Use **lazy evaluation** to avoid loading data into memory and further speed up queries.\n\n## References\n\n- [Polars documentation](https://docs.pola.rs/)\n- [Notes from a data witch: getting started with Apache Arrow](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}