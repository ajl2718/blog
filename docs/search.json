[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In my day job I am a data scientist who works with linked healthcare data for cancer research, applying machine learning and statistical methods to improve cancer outcomes.\nThis blog relates more to general tech, programming and data topics, rather than academia; the purpose is to record some interesting things I learn, which are hopefully useful to others too.\nMy background is in maths and physics and since around 2015 I have been in the data world, working on projects involving signal processing, geospatial analysis and over the past few years, clinical data science. My main interests are in data science, particularly the methodological aspects of statistics and machine learning, and also tools that can help to faciliate the grunt work that is common in data science (especially in the healthcare area).\n\nHere is a github project I’ve been working on for open-source geocoding, since I couldn’t find any good open-source geocoding libraries and prefer not to pay:\n\nwhereabouts: a fast open-source geocoding package built using Python and DuckDB that can be run in your own environment.\n\nSome other things I like: mountains; cycling; middle-eastern cooking experiments; reading; and music."
  },
  {
    "objectID": "posts/virtual-environments-with-jupyter/index.html",
    "href": "posts/virtual-environments-with-jupyter/index.html",
    "title": "Virtual environments with Jupyter Notebooks",
    "section": "",
    "text": "Virtual environments are a way of collecting all the packages you need for a given software project into one place, in such a way that it can be isolated from other software projects. For example, recently I have been experimenting with several different libraries for natural language processing and machine learning: Fast AI, PyTorch, spaCy. Installing all of them turns out to cause conflicts: one project may require one set of versions for the libraries and another requires a different set of versions.\nFurthermore, I mostly rely on Jupyter Notebooks, as they allow you to quickly experiment, which is a requirement for data science work. Since I keep forgetting how to set up a virtual environment that is accessible from Jupyter Notebook, I thought it would be a good idea to record this here to quickly remind myself."
  },
  {
    "objectID": "posts/virtual-environments-with-jupyter/index.html#create-the-environment-and-install-the-required-packages",
    "href": "posts/virtual-environments-with-jupyter/index.html#create-the-environment-and-install-the-required-packages",
    "title": "Virtual environments with Jupyter Notebooks",
    "section": "Create the environment and install the required packages",
    "text": "Create the environment and install the required packages\n\nFirstly make sure that conda is installed. The instructions are available here: https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html\nNext, create a virtual environment form conda. Here we will just call is ds_project1 and use Python 3.8:\n\nconda create -n ds_project1 python=3.8\n\nNow we can activate this environment to install are required packages. Here we’re installing FastAI:\n\nconda activate ds_project1\nconda install -c fastchan fastai\nThe string after -c indicates the repository where the package is located. This can be easily found through a Google search."
  },
  {
    "objectID": "posts/virtual-environments-with-jupyter/index.html#allow-jupyter-notebook-to-use-the-environment",
    "href": "posts/virtual-environments-with-jupyter/index.html#allow-jupyter-notebook-to-use-the-environment",
    "title": "Virtual environments with Jupyter Notebooks",
    "section": "Allow Jupyter Notebook to use the environment",
    "text": "Allow Jupyter Notebook to use the environment\nAfter all the required packages are added to the conda environment, we need to allow Jupyter to use it. Still in the virtual environment type:\nconda install -c anaconda ipykernel \npython -m ipykernel install --user --n=ds_project1\nNow each time you want to use this environment in Jupyter notebook, simply activate the environment and start jupyter notebook as you usually would:\nconda activate ds_project1\njupyter notebook\nNow when you want to create a new notebook the environment should be available from the dropdown list:\n\n\n\nDropdown list with the new conda environment"
  },
  {
    "objectID": "posts/virtual-environments-with-jupyter/index.html#removing-the-environment",
    "href": "posts/virtual-environments-with-jupyter/index.html#removing-the-environment",
    "title": "Virtual environments with Jupyter Notebooks",
    "section": "Removing the environment",
    "text": "Removing the environment\nTo remove the environment simply type:\nconda remove env --name ds_project1\nAnd then to remove the option from Jupyter notebook:\njupyter kernelspec uninstall ds_project1\n\nReferences\n\n\nConda Cheat Sheet\nFast AI\nHow to add your conda environment to your jupyter notebook in just 4 steps"
  },
  {
    "objectID": "posts/geocoding/index.html",
    "href": "posts/geocoding/index.html",
    "title": "A simple geocoder using Postgres and Python",
    "section": "",
    "text": "For a work project last year we needed to create at short notice a geocoder for Victorian addresses. While there are a number of open source options such as libpostal, we decided to take a different approach, following the ideas in the paper Exploiting Redundancy, Recurrence and Parallelism: How to Link Millions of Addresses with Ten Lines of Code in Ten Minutes. This allowed us to create a proof-of-concept within a few days and was relatively simple to code from scratch, using Python and Postgres.\n\nAn observation about street addresses\n\nThe paper is partly based on some observations about the statistics of street addresses which imply significant amount of redundancy in the way they are described. For example:\n\nBased on the January 2021 release of the GNAF there are 3,839,496 addresses in Victoria. 516,926 of which (i.e., over 13%) are uniquely defined by three numbers: flat number, street number and postcode.\n\nAs a result, these addresses can be identified irrespective of typographical errors in the street name, suburb or other text. Furthermore, even where the text is necessary to identify an address, often only a subset of the text is needed, and this is generally the parts of the text that occur relatively infrequently.\n\nThe algorithm: general idea\n\nThe authors of the paper use this idea as the starting point for a geocoding algorithm (actually an address linking algorithm in their paper) that has some nice properties:\n\nIt can be coded up in SQL in a few queries;\nThere is no need to standardize the addresses, i.e., to identify each of the address components and then re-write in a way that can be linked to the GNAF;\nIt can be scaled up to allow for fast linking of large datasets (as the title suggests).\n\nDespite its simplicity it works surprisingly well and at least for me was a good project for getting more hands-on experience using Postgres.\nBased on these ideas, the authors developed a simple geocoding algorithm that represents each address in terms of rare phrases, i.e., pairs of consecutive tokens that occur below a certain frequency, along with other infrequently occuring elements.\n\nThe algorithm: details and implementation\n\nHere are the basic steps of the algorithm for geocoding street addresses:\n\nCreate a database to store the GNAF data;\nFor both the input addresses and GNAF create corresponding phrase tables which generate the phrases for each address, i.e., the pairs of consecutive tokens;\nAn inverted index is generated on the phrases which maps each phrase to a set of addresses that contain these phrases;\nThe inverted index tables for the input addresses and GNAF addresses are then linked, only considering those that appear below a certain frequency. This has the advantage of only requiring searches over a small subset of addresses. The result is a set of candidate matches for each of the input addresses;\nFor each input address a similarity score is calculated with its set of candidate addresses. We used the similarity function from pg_tgrm module;\nAs a final step, an additional constraint on the numeric tokens is imposed: the set of numeric tokens in the input address has to be a subset of those in the matched address. This ensures, for example, that the unit numbers are correct.\n\nThe paper has SQL code for most of these steps, with the exception of the GNAF setup and the numerical constraint.\nI have written a Python module for geocoding that implements this algorithm in Postgres thanks to the nice results library. It is available in the repo PhraseGeocoder.\n\nSummary and extensions to the algorithm\n\nI found this an instructive project for learning more about Postgres, in particular things like Common Table Expressions, string similarity and operations that I wasn’t at all familiar with.\nThere are a few obvious ways that this algorithm can be extended:\n\nIntead of using pairs of consecutive tokens as phrases, use pairs of next-to-nearest neighbour phrases.\nTo better handle certain spelling errors, particularly those that occur in the text that is needed for the matching, construct phrases based on trigrams.\nChain together a sequence of geocoding algorithms, so that with higher data quality addresses can be geocoded by the faster phrase-based geocoder and the lower quality ones then get filtered out and geocoded by the slower but more accurate trigram based geocoder (or another variant).\n\nSome of these variations I have coded up and will upload them at a later date.\n\nReferences\n\n\nExploiting Redundancy, Recurrence and Parallelism: How to Link Millions of Addresses with Ten Lines of Code in Ten Minutes\nGithub repo for PhraseGeocoder"
  },
  {
    "objectID": "posts/local-geospatial-analysis/index.html",
    "href": "posts/local-geospatial-analysis/index.html",
    "title": "Geospatial analysis in a secure environment",
    "section": "",
    "text": "Sensitive data work involves stringent security constraints in the data science environments. For example, often I am forced to use virtual machines which are isolated from the public internet. This introduces many limitations to data science workflows, one in particular being the ability to perform geospatial analysis and visualisation. Packages like Folium require access to a web mapping service to download the map tiles for an area of interest. I therefore had to find a way around this.\nAfter some experimentation, I found a combination of contextily and geopandas were suitable as they allow you to download the map tiles and then plot them along with any data. There are two steps here:\n\nUse contextily to download the map tiles that are required for your application\nPlot the data and basemaps using geopandas.\n\nThe first step can lead to a large amount of data being downloaded (around 2Gb in my case).\n\nDownloading map tiles\nContextily requires the place name to be specified and uses OpenStreetMap’s Nominatim geocoder. This is suitable for geocoding suburbs, local government areas, states and other regions provided there are no spelling errors. Using a list of standard place names such as those from official government sources is therefore important.\nAs an example, the following code will download the map tile for the Local Government Area of Moreland in Australia.\nimport contextily as ctx\n\nlga = ctx.Place(\"Melbourne, Victoria, Australia\", \n                source=ctx.providers.CartoDB.DarkMatter, \n                path='data_folder/greater_melbourne_maptile.tiff')\nThere are a few different sources of freely available map tiles.\n\n\nVisualisation of data\nIt is now straightforward to write a script to loop through the areas of interest and download the corresponding map tile. Once the files are downloaded these can be plotted using contextily and geopandas. For example, using the SA2 shapefiles for greater melbourne we can plot each of these against our recently downloaded basemap, using shading based on the area.\nimport contextily as ctx\nimport geopandas\n\n%matplotlib inline\n\nax = sa2_gm.plot(figsize=(15, 15), linewidth=1, alpha=0.7, column='AREASQKM21')\nctx.add_basemap(ax, crs=sa2_gm.crs, source=f\"data_folder/greater_melbourne_maptile.tiff\")\nax.set_axis_off()\nAnd here is what the output should look like: \nThe notebook describing how to download the map tiles is available here and for visualisation here."
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html",
    "href": "posts/intro-to-webscraping/index.html",
    "title": "Intro to webscraping",
    "section": "",
    "text": "Webscraping involves writing small programs to automatically extract data from websites. It’s an extremely useful technique for creating large aggregated datasets using publicly available data as it’s inexpensive and fast. With the large amount of data available in the public domain it can also be the best way to obtain up-to-date and complete data for a given task.\nThere are plenty of applications of webscraping, across domains ranging from economic statistics to data journalism to compliance monitoring. Here are some specific ones:\nLearning the basics is straightforward and requires only a few lines of Python. The main challenge is identifying the relevant snippets of HTML, JSON or Javascript containing the data you are looking for; this is usually the most time-consuming part as well.\nIn this tutorial I will give an introduction to webscraping using two simple examples. All the code is in a Jupyter Notebook."
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html#ethics-and-best-practices",
    "href": "posts/intro-to-webscraping/index.html#ethics-and-best-practices",
    "title": "Intro to webscraping",
    "section": "Ethics and best practices",
    "text": "Ethics and best practices\nBefore we begin, there are some general tips to ensure you’re doing the right thing:\n\nRead the robots.txt file: this is found at (domain name of website)/robots.txt. It will tell you which parts of the website can and can’t be scraped;\nOnly make a single request to the website at a given time. This helps to avoid placing too much load on the server;\nRead the website Terms and Conditions, particularly if you are planning to use the data for commercial purposes.\n\nThe UK Office of National Statistics has a handy Webscraping Policy too. Unfortunately, no Australian jurisdiction appears to have created anything similar.\nNow, let’s begin."
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html#install-the-relevant-libraries",
    "href": "posts/intro-to-webscraping/index.html#install-the-relevant-libraries",
    "title": "Intro to webscraping",
    "section": "Install the relevant libraries",
    "text": "Install the relevant libraries\nWe’ll need Pandas and LXML for data wrangling and Requests to make HTTP requests.\npip install requests\npip install pandas\npip install lxml"
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html#make-your-first-http-request",
    "href": "posts/intro-to-webscraping/index.html#make-your-first-http-request",
    "title": "Intro to webscraping",
    "section": "Make your first HTTP request",
    "text": "Make your first HTTP request\nOpen up a Python terminal and type the following code\nimport requests\nurl = 'https://www.abc.net.au'\n\npage = requests.get(url)\nprint(f'Status: {page.status_code}')\nIf everything is working you should get:\n\nStatus: 200\n\nThere are a few different HTTP Verbs: GET, POST, DELETE, PUT and others. I have only ever used GET and POST (when you have to send some data). If you get a response code other than 200 then generally something has gone wrong. Here’s a full list of HTTP response codes."
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html#first-example-create-a-dataset-of-lonely-dogs",
    "href": "posts/intro-to-webscraping/index.html#first-example-create-a-dataset-of-lonely-dogs",
    "title": "Intro to webscraping",
    "section": "First example: Create a dataset of lonely dogs",
    "text": "First example: Create a dataset of lonely dogs\nIn this first example let’s scrape Pet Rescue, a site containing profiles of dogs and cats (and other animals) available for adoption. Perhaps we want a dataset of images of cats and dogs to train an object detection algorithm, or a dataset of common pet names, or maybe just want to know how the available pets are distributed throughout Australia.\n\nInspecting the data\nOnce you’re at the website, open up the Inspector (Tools → Web Developer → Inspector in Firefox) or Developer Tools (View → Developer → Developer Tools in Chrome).\n\n\n\nThe Inspector is located under Tools and Web Developer\n\n\nThen click on the ‘dogs’ link to get to the first page of ‘dog profiles’. You will see the window at the bottom under the Network tab fill up with links corresponding to different elements of the website. Note that the actual content will look slightly different as the pet listings change over time. Doing this we find the HTML snippet that relates to one of the dogs.\n\n\n\nSome cute dogs looking for a new home\n\n\nClick on the ‘dogs’ row under the Network tab (it should be the first row) and then Response and Response payload in the window on the right. This will show the HTML for the web page. We can do a quick search to get the name, location and characteristics of one of the dog profiles.\n\n\n\nThe HTML snippet corresponding to the entry for ‘Chase’\n\n\nThe entries for each of the dogs has the same structure and from this we can extract all the names, sizes and locations.\n\n\nUsing LXML to extract the relevant data\nNow we know where the relevant data is located, we can define some LXML paths to extract it. An alternative is Beautiful Soup. Here are LXML paths for the name and location data:\nname_path = '//article[@class=\"cards-listings-preview\"]/a/header/h3/text()'\nlocation_path = '//strong[@class=\"cards-listings-preview__content__section__location\"]/text()'\nThe LXML syntax can appear a bit clunky but the name_path variable is saying to look for every instance of the article tag with class value “cards-listing-preview” then look at the a tag below this, followed by the header tag, the h3 tag and then the text within this.\nThe code for extracting the names and locations is then:\nfrom lxml import html\nimport requests\n\nurl = 'https://www.petrescue.com.au/listings/search/dogs'\npage = requests.get(url)\n\ntree = html.fromstring(page.text)\n\nnames = tree.xpath(name_path)\nlocations = tree.xpath(location_path)\nBecause of the &lt;i&gt; tags we get additional strings returned in the location_path so we take every second one. The full Python code to get all the data from a single page is then:\nfrom lxml import html\nimport requests\n\nurl = 'https://www.petrescue.com.au/listings/search/dogs'\npage = requests.get(url)\n\ntree = html.fromstring(page.text)\n\nname_path = '//article[@class=\"cards-listings-preview\"]/a/header/h3/text()'\nlocation_path = '//strong[@class=\"cards-listings-preview__content__section__location\"]/text()'\n\nnames = tree.xpath(name_path)\nlocations = tree.xpath(location_path)\n\nlocations = locations[1::2]\nSkipping to the next page, the url is https://www.petrescue.com.au/listings/search/dogs?page=2 so we can scrape all the pages by looping over a range of integers and, for each request, appending an integer to the base url https://www.petrescue.com.au/listings/search/dogs?page=. Let’s add in this bit of code:\nfrom lxml import html\nimport requests\nimport pandas as pd\n\nurl_base = 'https://www.petrescue.com.au/listings/search/dogs?page='\n\nname_path = '//article[@class=\"cards-listings-preview\"]/a/header/h3/text()'\nlocation_path = '//strong[@class=\"cards-listings-preview__content__section__location\"]/text()'\n\nall_names = []\nall_locations = []\n\nfor n in range(1, 50):\n    print(f'Scraping page: {n}')\n    url = f'{url_base}{n}'\n    page = requests.get(url)\n    tree = html.fromstring(page.text)\n    names = tree.xpath(name_path)\n    locations = tree.xpath(location_path)\n    locations = locations[1::2]\n    all_names += names\n    all_locations += locations\nI like to print out which page is being scraped for debugging purposes and also as a sort of progress indicator. Finally, once we’ve scraped all this data, we can add put it all into a nice tidy Pandas Dataframe (with a tiny bit of text processing to remove unwanted spaces and new lines characters):\ndf = pd.DataFrame(data={'name': all_names, 'location': all_locations})\ndf['name'] = df['name'].str.strip()\ndf['location'] = df['location'].str.strip()\nAnd we have a nice tidy dataset of dog names and locations. Here are the first five rows:\n\n\n\nname\nlocation\n\n\n\n\nTeddy Yoric\nBrunswick, VIC\n\n\nTilly Goldsworthy\nRichmond, VIC\n\n\nMarnie & Panda Wazowski\nHampton, VIC\n\n\nBear Hartwell\nAltona Meadows, VIC\n\n\nShayla Caballero\nClifton Hill, VIC"
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html#second-example-atm-locations-in-australia",
    "href": "posts/intro-to-webscraping/index.html#second-example-atm-locations-in-australia",
    "title": "Intro to webscraping",
    "section": "Second example: ATM locations in Australia",
    "text": "Second example: ATM locations in Australia\nFor this example we’ll extract JSON data, which is usually a bit easier to work with than HTML as it’s naturally represented as Python dictionaries. We’ll do this by scraping all National Australia Bank ATMs.\nFirstly, with the developer tools / inspector open, navigate to https://www.nab.com.au/locations. In the inspector select the XHR tab in the right-hand pane. The reasoning is that, since there is a store locator in the page, there is an API behind the scenes that is delivering this data.\nScrolling through the different files we inspect the response provided by each. We find that file ‘4000?v=1’ produces a nice nested data structure of what appears to be locations and names of ATMs and branches. Clicking through to the headers tab we find the URL is:\nhttps://api.nab.com.au/info/nab/location/locationType/atm+brc/queryType/geo/-37.7787919055083/144.92910033652242/-37.74062261073386/144.99833566347752/1/4000?v=1\nThere is also some header information that we may need, so we include this too.\nNotice that four of the numbers in the URL look like latitude and longitude values and so these likely describe a bounding box for the search area. Since we are interested in all the ATMs and branches in the country we expand it to a box that encompasses all of Australia. The maximum and minimum latitude and longitude values of this bounding box are:\nlat_min, lng_min = -43.834124, 114.078644\nlat_max, lng_max = -10.400824, 154.508331\nThe code to scrape the locations of all NAB ATMS (which can in this case be done in only a single request) is:\nimport requests\nimport pandas as pd\n\nlat_min, lng_min = -43.834124, 114.078644\nlat_max, lng_max = -10.400824, 154.508331\n\nurl = f'https://api.nab.com.au/info/nab/location/locationType/atm+brc/queryType/geo/{lat_min}/{lng_min}/{lat_max}/{lng_max}/1/4000?v=1'\n\nheaders = {'Host': 'api.nab.com.au', \n'Origin': 'https://www.nab.com.au', \n'Referer': 'https://www.nab.com.au/',\n'x-nab-key': 'a8469c09-22f8-45c1-a0aa-4178438481ef'}\n\npage = requests.get(url=url, headers=headers)\ndata = page.json()\nNotice that since the output of the HTTP request is in JSON format it can immediately be converted to a Python dict. It is now a matter of finding where the location information is.\nThe final Python code is:\nimport requests\nimport pandas as pd\n\nlat_min, lng_min = -43.834124, 114.078644\nlat_max, lng_max = -10.400824, 154.508331\n\nurl = f'https://api.nab.com.au/info/nab/location/locationType/atm+brc/queryType/geo/{lat_min}/{lng_min}/{lat_max}/{lng_max}/1/4000?v=1'\n\nheaders = {'Host': 'api.nab.com.au', \n'Origin': 'https://www.nab.com.au', \n'Referer': 'https://www.nab.com.au/',\n'x-nab-key': 'a8469c09-22f8-45c1-a0aa-4178438481ef'}\n\npage = requests.get(url=url, headers=headers)\ndata = page.json()\n\ndf = pd.json_normalize(data['locationSearchResponse']['locations'])\ndf = df[['atm.address1', 'atm.suburb', 'atm.state', 'atm.postcode', 'atm.latitude', 'atm.longitude']].dropna()\nThe json_normalize method in Pandas is a handy way to flatten the nested JSON data into a flat data structure. Now we have a nice tidy data frame with the address, latitude and longitude of each ATM:\n\n\n\n\n\n\n\n\n\n\n\natm.address1\natm.suburb\natm.state\natm.postcode\natm.latitude\natm.longitude\n\n\n\n\nBrunswick City Centre, 94 Sydney Road\nBrunswick\nVIC\n3056\n-37.775660\n144.961047\n\n\n406 Sydney Road\nCoburg\nVIC\n3058\n-37.743756\n144.966389\n\n\n406 Sydney Road\nCoburg\nVIC\n3058\n-37.743756\n144.966389"
  },
  {
    "objectID": "posts/intro-to-webscraping/index.html#summary-and-general-tips",
    "href": "posts/intro-to-webscraping/index.html#summary-and-general-tips",
    "title": "Intro to webscraping",
    "section": "Summary and General tips",
    "text": "Summary and General tips\nThat’s enough of an intro to get started. Using a couple of examples, we’ve covered the basics of scraping HTML and JSON, and parsing the data into tidy form using LXML and Pandas. To summarise:\n\nRemember to check the Terms and Conditions, robots.txt file and consider the application that you are using the data for;\nThe developer tools in the browser are a good way to identify the relevant elements in the web site;\nThere is fair amount of hack-work required to find where the relevant data is, and it changes from website to website. The two examples here illustrate how scraping works for a fair number of different websites;\nThose with store locators or unofficial APIs are generally much easier to scrape as the data is already in a relatively structured form.\n\nTo make things easy, all the code in this post is available in a (hopefully) easy-to-follow Jupyter Notebook."
  },
  {
    "objectID": "posts/using-docker/index.html",
    "href": "posts/using-docker/index.html",
    "title": "Setting up a scratch space using Poetry, Docker and Jupyter",
    "section": "",
    "text": "Often I want to experiment with a new Python package but don’t want to have to set up a virtual environment, or affect the packages that I already have installed on my system. Jupyter Notebooks provide a great interface for quickly testing out new features of a library but using them inside a virtual environment can also be a pain.\nRunning a Jupyter Notebook inside a Docker container is a nice solution to this problem. This post gathers together my understanding of Docker in setting this up. I’m still very much learning about Docker and haven’t had many other opportunities to really see where it is essential but this is one simple use case, and a good intro for me to learn about it.\nThe Docker documentation has very useful and easy to follow tutorials to begin using Docker."
  },
  {
    "objectID": "posts/using-docker/index.html#set-up-a-project-using-poetry",
    "href": "posts/using-docker/index.html#set-up-a-project-using-poetry",
    "title": "Setting up a scratch space using Poetry, Docker and Jupyter",
    "section": "Set up a project using Poetry",
    "text": "Set up a project using Poetry\nPoetry is handy for setting up the folder structure for a project, including tests, modules, a README.md and managing dependencies\npoetry new (project_name)\nNow enter into the Poetry shell and add the package that you’re interested in testing out\npoetry shell\npoetry add (package_name)\nAnd remember to add Jupyter notebook\npoetry add jupyter\nFinally export a requirements.txt file that we can use to create the Docker image.\npoetry export -f requirements.txt --without-hashes --output requirements.txt"
  },
  {
    "objectID": "posts/using-docker/index.html#create-a-dockerfile",
    "href": "posts/using-docker/index.html#create-a-dockerfile",
    "title": "Setting up a scratch space using Poetry, Docker and Jupyter",
    "section": "Create a Dockerfile",
    "text": "Create a Dockerfile\nCreate a file Dockerfile (without any extension) in the working directory of the project. We use a Python 3.8 base image and then add the requirements.txt file with the necessary libraries.\nFROM python:3.8-slim-buster\nWORKDIR /app\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"jupyter\", \"notebook\", \"--port=8888\", \"--no-browser\", \"--ip=0.0.0.0\", \"--allow-root\"]"
  },
  {
    "objectID": "posts/using-docker/index.html#build-the-docker-image-and-then-run-it-to-test-out-the-library-in-the-notebook",
    "href": "posts/using-docker/index.html#build-the-docker-image-and-then-run-it-to-test-out-the-library-in-the-notebook",
    "title": "Setting up a scratch space using Poetry, Docker and Jupyter",
    "section": "Build the Docker image and then run it to test out the library in the notebook",
    "text": "Build the Docker image and then run it to test out the library in the notebook\nIn the same working directory build the image:\ndocker build --tag my_test_enviroment .\nand run it to start a container\ndocker run -p 8888:8888 my_test_environment\nNow you can open up a browser and create a new notebook which will have the necessary libraries from the Docker image. Because this is run inside a container, no data will be stored between stopping and running the container again so any notebooks cannot be saved."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html",
    "href": "posts/large_datasets_in_python/index.html",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "",
    "text": "While Pandas is still the go-to Python package for data analysis, there are now a new generation of data analysis tools which are much faster and better suited to working with data that is larger than the available memory. It isn’t unusual to encounter a dataset around 10Gb in size which is usually just large enough to be awkward to work with on a single machine.\nThis article is a summary of some tinkering with these tools. These turn out to be very useful for the kinds of data that I use in my day job. The goal is to answer the questions:\nIn addition to Polars, there are other tools in this area, including DuckDB and Ibis. And if you are using R there is an excellent tutorial on how to use the Arrow package for working with large datasets."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#querying-data-in-pandas-the-messy-way-and-the-pretty-way",
    "href": "posts/large_datasets_in_python/index.html#querying-data-in-pandas-the-messy-way-and-the-pretty-way",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Querying data in Pandas: the messy way and the pretty way",
    "text": "Querying data in Pandas: the messy way and the pretty way\nFirst let’s download a largish dataset. GNAF Core is a quarterly-updated dataset containing all street addresses in Australia. It’s a single csv (well actually psv file) around 3.2Gb in size, so can fit in memory and analysed using Pandas.\nLet’s load data and carry out some simple transformations.\nimport pandas as pd\ngnaf = pd.read_csv('gnaf_core.psv', sep='|')\ngnaf.shape\nThis is a dataframe with 15.6 million rows and 27 columns. Let’s group the number of addresses in postcode 3000 (City of Melbourne) by street name and count how many there are.\nOne way to do this is as follows:\n# filter to postcode 3000\ngnaf_melbourne = gnaf[gnaf['POSTCODE'] == 3000]\n\n# count the number of addresses per street in postcode 3000\nmelbourne_street_counts = gnaf_melbourne.groupby('STREET_NAME').size()\n\n# sort the data in descending order by the number of addresses\nmelbourne_street_counts.sort_values(ascending=False)\nWe should see that the top 5 streets are LONSDALE, ABECKETT, COLLINS, LITTLE LONSDALE, LA TROBE which have over 7000 addresses, while streets like HEAPE, HARWOOD, RACING CLUB and DOMAIN each have only one address.\nA nicer way of writing this code is to chain these statements into a single query. I learned this from reading Matt Harrison’s tweets and articles.\ngnaf.query('POSTCODE == 3000').groupby('STREET_NAME').size().sort_values(ascending=False)\nNotice that this uses the query method to filter the addresses and then a sequence of methods to carry the remaining operations. In a slightly more readable form\n(\n    gnaf\n    .query('POSTCODE == 3000')\n    .groupby('STREET_NAME')\n    .size()\n    .sort_values(ascending=False)\n)\nThis sequence of transformation now reads as a single ‘recipe’ with each transformation appearing in a single line, making it all much easier to interpret. It more closely resembles dplyr code in R and as we will see later, Polars code. It also avoids creating unneccessary variables and memory in the intermediate steps. So far, so good.\nLet’s answer another question: What is the distribution of starting letters for streets in New South Wales?\n(\n    gnaf\n    .query('STATE == \"NSW\"')\n    .STREET_NAME\n    .str.slice(0, 1)\n    .value_counts()\n)\nAnd the most common starting letter for a street name is B with 383,483 addresses. So now we can write slick, easy to read queries in Pandas, and this leads us to the main subject of this article: Polars."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#why-polars",
    "href": "posts/large_datasets_in_python/index.html#why-polars",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Why Polars?",
    "text": "Why Polars?\nI have only recently started playing around with Polars but it’s proving to be a better alternative to Pandas for large datasets. I am definitely not an expert in this area, and there are other packages such as Ibis and DuckDB are being widely used as well.\nThe API for Polars may feel a bit unintuitive at first, but less so once you’re familiar with the method chaining approach to writing Pandas transformations as described above. In any case, it’s all well documented on the Polars webpage\nimport polars as pl\n\ngnaf = pl.read_csv('GNAF_CORE.psv', separator='|', infer_schema_length=10000)\ngnaf.shape\nThe first thing you should notice is the speed in loading the csv. We’ll do a comparison with Pandas later.\n(\n    gnaf\n    .filter(pl.col(\"POSTCODE\") == 3000)\n    .group_by(\"STREET_NAME\")\n    .count()\n    .sort('count', descending=True)\n)\nAside from some annoying differences in syntax (e.g., using descending instead of ascending to sort the dataframe), the overall code looks very similar. And now for the second transformation that counts the number of street names in New South Wales starting with different letters.\n(\n    gnaf\n    .filter(pl.col('STATE') == \"NSW\")\n    .select(pl.col(\"STREET_NAME\"))\n    .with_columns(pl.col(\"STREET_NAME\").str.slice(0, 1).alias(\"letter\"))\n    .group_by('letter')\n    .count()\n    .sort('count', descending=True)\n)\nOk, not quite as elegant, especially with the with_columns syntax but it gives the same results."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#comparing-the-speeds",
    "href": "posts/large_datasets_in_python/index.html#comparing-the-speeds",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Comparing the speeds",
    "text": "Comparing the speeds\nSo loading the csv file was faster, let’s compare how fast\nt1 = time()\ngnaf = pl.read_csv(filename, separator='|', infer_schema_length=10000)\nt2 = time()\nprint(t2-t1)\n\nt1 = time()\ngnaf = pd.read_csv(filename, sep='|')\nt2 = time()\nprint(t2-t1)\nOn my Macbook M3, Polars takes around 1.7s and Pandas around 39.1s, so Polars is over 20 times faster at loading this csv file!\nt1 = time()\n(\n    gnaf\n    .filter(pl.col('STATE') == \"NSW\")\n    .select(pl.col(\"STREET_NAME\"))\n    .with_columns(pl.col(\"STREET_NAME\").str.slice(0, 1).alias(\"letter\"))\n    .group_by('letter')\n    .count()\n    .sort('count', descending=True)\n)\nt2 = time()\nprint(t2-t1)\n\nt1 = time()\n(\n    gnaf\n    .query('STATE == \"NSW\"')\n    .STREET_NAME\n    .str.slice(0, 1)\n    .value_counts()\n)\nt2 = time()\nprint(t2-t1)\nAround 1.3s in Polars and 2.4s in Pandas.\nt1 = time()\n(\n    gnaf\n    .filter(pl.col(\"POSTCODE\") == 3000)\n    .group_by(\"STREET_NAME\")\n    .len()\n    .sort('len', descending=True)\n)\nt2 = time()\nprint(t2 - t1)\n\ngnaf = pd.read_csv(filename, sep='|')\n\nt1 = time()\nresults = (\n    gnaf\n    .query('POSTCODE == 3000')\n    .groupby('STREET_NAME')\n    .size()\n    .sort_values(ascending=False)\n)\nt2 = time()\nprint(t2-t1)\n1.39s in Polars and 0.43s in Pandas. So Pandas is the winner here."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#another-speed-up-parquet-files",
    "href": "posts/large_datasets_in_python/index.html#another-speed-up-parquet-files",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Another speed up: parquet files",
    "text": "Another speed up: parquet files\nRather than using csv files we can convert them to parquet. This is a binary file format that represents tabular datasets as columns rather than rows; both of these properties result in smaller file sizes that are quicker to load and faster to analyse. A more in-depth explanation for why this is the case can be found in this article by Wes McKinney, the creator of Pandas.\nThe original file is 3.2Gb in csv format but when converted to parquet it’s 970Mb. This is a decent saving on space. From experience Parquet files can be up to 10% the size of csv file.\n# load in pandas\nt1 = time()\ngnaf = pd.read_parquet(filename)\nt2 = time()\nprint(t2-t1)\n\n# load in polars\nt1 = time()\ngnaf = pl.read_parquet(filename)\nt2 = time()\nprint(t2-t1)\n1.3s in Polars and 14.1s in Pandas."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#lazy-evaluation",
    "href": "posts/large_datasets_in_python/index.html#lazy-evaluation",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Lazy evaluation",
    "text": "Lazy evaluation\nThe real benefit of using polars is that queries can be executed using so-called lazy evaluation, rather than loading the data into memory first.\nThis involves the use of scan_parquet or scan_csv which essentially creates a pointer to the file on the disk.\ngnaf = pl.scan_parquet(filename)\nt1 = time()\n(\n    gnaf\n    .filter(pl.col(\"POSTCODE\") == 3000)\n    .group_by(\"STREET_NAME\")\n    .len()\n    .sort('len', descending=True)\n    .collect()\n)\nt2 = time()\nprint(t2 - t1)\nThe only difference is that a collect() method is appended to the end of the query so that it actually runs. The result: only 0.08 seconds! I was surprised about how much faster this is. Let’s see how fast the earlier query runs:\nt1 = time()\n(\n    gnaf\n    .filter(pl.col('STATE') == \"NSW\")\n    .select(pl.col(\"STREET_NAME\"))\n    .with_columns(pl.col(\"STREET_NAME\").str.slice(0, 1).alias(\"letter\"))\n    .group_by('letter')\n    .len()\n    .sort('len', descending=True)\n    .collect()\n)\nt2 = time()\nprint(t2-t1)\nWhich takes 0.25 seconds."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#larger-than-memory-datasets",
    "href": "posts/large_datasets_in_python/index.html#larger-than-memory-datasets",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Larger than memory datasets",
    "text": "Larger than memory datasets\nSince the main point of this article is to better understand how to work with larger than memory data in Python, let’s look at a collection of datasets from New York City taxi trips. From 2011 to 2023 these files total to around 20Gb and are forunately already in parquet format.\nSome work is needed to transform the data so that each file has the same column names and datatypes. This is done in this notebook. We can then ‘lazy load’ all the parquet files as follows\nnyc_yaxis = pl.scan_parquet('taxi_trips/*.parquet')\nAnd we can do some basic queries\n# how many rows?\nnyc_taxis.select(pl.len()).collect()\n\n# what is the median fare?\nnyc_taxis.select(pl.median('fare_amount')).collect()\n\n# what is the distribution of passenger counts?\n(\n    nyc_taxis.group_by('passengder_count')\n    .len()\n    .sort('len', descending=True)\n    .collect()\n    .head(10)\n)\nThere are around 1.4 billion trips, with a median fare of $9. These queries are all run out of the memory so can take 10s or so.\nFinally let’s consider a more interesting question: how do credit card vs cash payments vary over time?\npayment_types_year = (\n    nyc_taxis\n    .with_columns(pl.col('tpep_pickup_datetime').dt.year().alias('pickup_year'))\n    .group_by(pl.col(['pickup_year', 'payment_type']))\n    .len()\n    .filter(pl.col('pickup_year').is_in(list(range(2011,2024))))\n    .filter(pl.col('payment_type').is_in([1,2]))\n    .sort(pl.col('pickup_year'))\n    .collect()\n)\n\nplt.figure(figsize=(6, 4))\nsns.lineplot(data=payment_types_year.to_pandas(), x='pickup_year', y='len', hue='payment_type')\nplt.xlabel('Year')\nplt.ylabel('Number of trips')\nplt.show()\n\n\n\n\n\nWhich illustrates some of the trends driven by the adoption of Uber; the COVID-19 pandemic; and the increasing use of credit card payments in lieu of cash."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#lessons",
    "href": "posts/large_datasets_in_python/index.html#lessons",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "Lessons",
    "text": "Lessons\nTo summarise:\n\nUse parquet files rather than csv where possible to reduce storage and speed up querying.\nIf using pandas for data analysis, write data transformations as single queries involving method chaining, rather than in multiple steps. This not only makes code easier to read, but easier to debug and removes so-called ‘side-effects.’\nSwitch to a package like Polars to significantly speed up data analysis workflows.\nUse lazy evaluation to avoid loading data into memory and further speed up queries."
  },
  {
    "objectID": "posts/large_datasets_in_python/index.html#references",
    "href": "posts/large_datasets_in_python/index.html#references",
    "title": "Larger than memory datasets in Python using Polars",
    "section": "References",
    "text": "References\n\nPolars documentation\nNotes from a data witch: getting started with Apache Arrow"
  },
  {
    "objectID": "posts/regular-lattice-shapefiles/index.html",
    "href": "posts/regular-lattice-shapefiles/index.html",
    "title": "Regular lattice shapefiles with geopandas",
    "section": "",
    "text": "For a project a while ago I wanted to create shapefiles of regular lattices (as opposed to the SA1 / SA2 boundaries that often are used in Austraila). For example a lattice of hexagons where each hexagon has side length 1km. Or alternatively a lattice of squares. Furthermore, it is often useful to be able to estimate a quantity in a new geometry, based on the quantity is another geometry. For example, we may want to estimate the population in each hexagon based on the values in the intersecting SA1 or SA2 polygons.\nI created three Jupyter notebooks to carry out these tasks:\n\nThe first generates a shapefile consisting of regular hexagons covering a particular area of interest;\nThe second is similar to the above but contains code for generating both hexagonal and square lattices;\nThe third one shows how to estimate the population contained in each of the hexagons based on the 2016 Census counts at the SA1 level.\n\nAnd here is a nice picture of a hexgrid:\n\n\n\nA hexgrid covering Victoria"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nMar 31, 2024\n\n\nLarger than memory datasets in Python using Polars\n\n\npython, polars, large data\n\n\n\n\nJul 9, 2022\n\n\nVirtual environments with Jupyter Notebooks\n\n\npython, jupyter\n\n\n\n\nMay 14, 2022\n\n\nGeospatial analysis in a secure environment\n\n\npython, geospatial\n\n\n\n\nApr 2, 2021\n\n\nSetting up a scratch space using Poetry, Docker and Jupyter\n\n\npython, docker, jupyter\n\n\n\n\nMar 14, 2021\n\n\nA simple geocoder using Postgres and Python\n\n\npython, postgres, geospatial\n\n\n\n\nJan 1, 2021\n\n\nIntro to webscraping\n\n\npython, webscraping, requests, open data\n\n\n\n\nDec 31, 2020\n\n\nRegular lattice shapefiles with geopandas\n\n\npython, geopandas, geospatial visualisation\n\n\n\n\n\nNo matching items"
  }
]